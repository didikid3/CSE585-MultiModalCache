{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c197b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d915cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "full_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# embedding_layer exposes the ViT embedding utilities (patch projection, cls token, etc.)\n",
    "embedding_layer = full_model.vit.embeddings\n",
    "# model hidden size (channels for patch embeddings)\n",
    "hidden_size = full_model.config.hidden_size\n",
    "# patch size (typically 16 for this model)\n",
    "patch_size = getattr(full_model.config, 'patch_size', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97000f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCompressor(nn.Module):\n",
    "    \"\"\"Compress ViT patch embeddings (C x Gh x Gw) -> (C' x Gh x Gw).\n",
    "    This module can accept either precomputed patch embeddings or raw ViT pixel inputs\n",
    "    and will use the provided `embedding_layer` to compute patches when given pixel tensors.\"\"\"\n",
    "    def __init__(self, embedding_layer=None, in_channels=None, compressed_channels=64):\n",
    "        super().__init__()\n",
    "        # embedding_layer: the ViT embeddings module (full_model.vit.embeddings)\n",
    "        self.embedding_layer = embedding_layer\n",
    "        # determine in_channels from either provided value or embedding_layer config\n",
    "        if in_channels is None and embedding_layer is not None:\n",
    "            try:\n",
    "                in_channels = embedding_layer.position_embeddings.shape[-1]\n",
    "            except Exception:\n",
    "                # fall back to None â€” user should provide in_channels\n",
    "                in_channels = None\n",
    "        if in_channels is None:\n",
    "            raise ValueError('in_channels must be provided if embedding_layer does not expose channel size')\n",
    "\n",
    "        # 1x1 convs act as channel compressors while preserving spatial layout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, compressed_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(compressed_channels, compressed_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def _patch_embeddings_from_pixels(self, pixel_values):\n",
    "        # Use the supplied embedding_layer to obtain patch embeddings from pixels\n",
    "        # embedding_layer.patch_embeddings should accept pixel_values and return either\n",
    "        # (B, N, C) or (B, C, Gh, Gw). We normalize to (B, C, Gh, Gw).\n",
    "        patch_emb = self.embedding_layer.patch_embeddings(pixel_values)\n",
    "        if patch_emb.ndim == 3:\n",
    "            B, N, C = patch_emb.shape\n",
    "            G = int(math.sqrt(N))\n",
    "            patch_emb = patch_emb.transpose(1, 2).reshape(B, C, G, G)\n",
    "        elif patch_emb.ndim == 4:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected patch_emb shape: {patch_emb.shape}')\n",
    "        return patch_emb\n",
    "\n",
    "    def forward(self, x, is_pixel_values=False):\n",
    "        \"\"\"If `is_pixel_values` is True, `x` should be pixel_values (B,3,H,W),\n",
    "        otherwise `x` should be patch embeddings (B,N,C) or (B,C,Gh,Gw).\n",
    "        Returns compressed feature map with shape (B, C', Gh, Gw).\"\"\"\n",
    "        if is_pixel_values:\n",
    "            if self.embedding_layer is None:\n",
    "                raise RuntimeError('No embedding_layer available to compute patch embeddings from pixels')\n",
    "            patch_emb = self._patch_embeddings_from_pixels(x)\n",
    "        else:\n",
    "            patch_emb = x\n",
    "            if patch_emb.ndim == 3:\n",
    "                B, N, C = patch_emb.shape\n",
    "                G = int(math.sqrt(N))\n",
    "                patch_emb = patch_emb.transpose(1, 2).reshape(B, C, G, G)\n",
    "            elif patch_emb.ndim == 4:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Unexpected patch_emb shape: {patch_emb.shape}')\n",
    "        return self.net(patch_emb)\n",
    "\n",
    "# create a compressor instance (adjust compressed_channels as needed).\n",
    "# If embedding_layer is provided we can infer in_channels from the model config (hidden_size).\n",
    "compressor = PatchCompressor(embedding_layer=embedding_layer, in_channels=hidden_size, compressed_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path, return_vector=True, device=None):\n",
    "    \"\"\"Load an image, compute ViT patch embeddings (via compressor), compress with a small CNN,\n",
    "    and return either the compressed feature map or flattened vector.\"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(images=image, return_tensors='pt')\n",
    "    pixel_values = inputs.get('pixel_values')  # (B, 3, H, W)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    compressor.to(device)\n",
    "\n",
    "    compressed_map = compressor(pixel_values, is_pixel_values=True)\n",
    "\n",
    "    if return_vector:\n",
    "        # flatten spatial grid into a single vector per image\n",
    "        return compressed_map.flatten(start_dim=1).cpu()\n",
    "    return compressed_map.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fcead67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchCompressor(\n",
       "  (embedding_layer): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.eval()\n",
    "compressor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41700db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity helpers: cosine, L2 and a convenience compare_images() function\n",
    "def cosine_similarity(a, b, eps=1e-8):\n",
    "    \"\"\"Compute cosine similarity between two vectors or batches.\"\"\"\n",
    "    # accept 1D or 2D tensors; return scalar for single pair or 1D tensor for batch\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    if a.ndim == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "        b = b.unsqueeze(0)\n",
    "    a = a.float()\n",
    "    b = b.float()\n",
    "    a_norm = a / (a.norm(dim=1, keepdim=True) + eps)\n",
    "    b_norm = b / (b.norm(dim=1, keepdim=True) + eps)\n",
    "    sims = (a_norm * b_norm).sum(dim=1)\n",
    "    return sims.item() if sims.numel() == 1 else sims\n",
    "\n",
    "def l2_distance(a, b):\n",
    "    \"\"\"Compute L2 (Euclidean) distance between two vectors or batches.\"\"\"\n",
    "    a = a.squeeze()\n",
    "    b = b.squeeze()\n",
    "    if a.ndim == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "        b = b.unsqueeze(0)\n",
    "    d = (a - b).norm(p=2, dim=1)\n",
    "    return d.item() if d.numel() == 1 else d\n",
    "\n",
    "def compare_images(path1, path2, method='cosine', device=None):\n",
    "    \"\"\"Compute similarity between two image paths using compressed embeddings.\"\"\"\n",
    "    # Obtain flattened compressed embeddings (shape: (1, D) or (D,))\n",
    "    e1 = get_image_embedding(path1, return_vector=True, device=device)\n",
    "    e2 = get_image_embedding(path2, return_vector=True, device=device)\n",
    "\n",
    "    if method == 'cosine':\n",
    "        return cosine_similarity(e1, e2)\n",
    "    elif method == 'l2':\n",
    "        return l2_distance(e1, e2)\n",
    "    elif method == 'dot':\n",
    "        # raw dot product (may be useful if vectors are not normalized)\n",
    "        v1 = e1.squeeze()\n",
    "        v2 = e2.squeeze()\n",
    "        if v1.ndim == 2:\n",
    "            v1 = v1.squeeze(0)\n",
    "            v2 = v2.squeeze(0)\n",
    "        return (v1 * v2).sum().item()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {method}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.5283985137939453\n",
      "L2 distance: 16.557477951049805\n",
      "Dot product: 153.58360290527344\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    \"images/lotr/img (12).jpg\",\n",
    "    \"images/lotr/img (100).jpg\",\n",
    "    \"images/lotr/img (101).jpg\",\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    sim_cos = compare_images(image_paths[0], image_paths[1], method='cosine')\n",
    "    dist_l2 = compare_images(image_paths[0], image_paths[1], method='l2')\n",
    "    dot_prod = compare_images(image_paths[0], image_paths[1], method='dot')\n",
    "\n",
    "print('Cosine similarity:', sim_cos)\n",
    "print('L2 distance:', dist_l2)\n",
    "print('Dot product:', dot_prod)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
